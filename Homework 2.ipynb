{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "The gradient of this function $f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2$ is\n",
    "$$\n",
    "g^T = \n",
    "\\begin{bmatrix}\n",
    "4x_{1} - 4x_{2}\\\\\n",
    "-4x_{1} + 3x_{2} + 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "then we can have the Hessian martix, which is\n",
    "$$\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "4&-4\\\\\n",
    "-4&3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Let's calculate the eigenvalue for the Hessian matrix, which is $\\lambda_{1} = \\frac{-7+\\sqrt{65}}{2}$ and $\\lambda_{2} = \\frac{-7-\\sqrt{65}}{2}$. It shows that one eigenvalue is positive, the other is negative. So the stationary point of this function is a saddle point.\n",
    "\n",
    "Use Tanlor's expansion to expand the function, we can have ($x_{0}$is the saddle point)\n",
    "$$\n",
    "f(x) = f(x_{0}) + g^T(x_{0})(x-x_{0})+\\frac{1}{2}(x-x_{0})^T H (x-x_{0})\n",
    "$$\n",
    "We also know that $g^T(x_{0}) = 0$, so the saddle point is $x_{1}=x_{2}=1$ and $f(x)-f(x_{0})=\\frac{1}{2}(x-x_{0})^T H (x-x_{0})$\n",
    "\n",
    "Decompose the expression $f(x)-f(x_{0})=\\frac{1}{2}(x-x_{0})^T H (x-x_{0})$, we can have \n",
    "$$\n",
    "f(x)-f(x_{0})=\\frac{1}{2}(4(x_{1}-x_{2})^2-(x_{2}-1)^2)\n",
    "$$\n",
    "\n",
    "The downslopes should meet $f(x)-f(x_{0}) < 0$. So we can have that $(x_{1}-x_{2})^2 < \\frac{1}{4}(x_{2}-1)^2$, which is the downslope direction away from the saddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "We minimize the distance between point in the plance and point $(-1, 0, 1)^T$ with the constraint\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{minimize:} && (x_1+1)^2+x_2^2+(x_3-1)^2\\\\\n",
    "&\\text{subject to:} && x_1 + 2x_2 + 3x_3 = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's convert the constraint as $x_1 = 1 - 2x_2 - 3x_3$, plug it into the minimized function:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{minimize:} &&f=(2-2x_2-3x_3)^2+x_2^2+(x_3-1)^2=5x_2^2-8x_2+12x_2x_3-14x_3+10x_3^2+5\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "We can calculate the Hessian matrix for the function $f$ and find\n",
    "$$\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "10&12\\\\\n",
    "12&20\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The eigenvalue is $\\lambda_1 = 28$ and $\\lambda_2 = 2$, which shows that $H$ is positive definite matrix. From the Lemma, we know that function $f$ is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point in the plane is x1 = [-1.07142857] x2 = [-0.14244893] x3 = [0.78544215]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the gradient descent algorithm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def obj(x):\n",
    "    return 5 * x[0] ** 2 + 10 * x[1] ** 2 + 12 * x[0] * x[1] - 8 * x[0] - 14 * x[1] + 5\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([10 * x[0] + 12 * x[1] - 8, 20 * x[1] + 12 * x[0] - 14])\n",
    "\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = np.array([[0], [0]])  # initial guess\n",
    "iter = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[iter]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x), ord=2)\n",
    "\n",
    "error_list = list()\n",
    "iter_list = list()\n",
    "error_list.append(error)\n",
    "iter_list.append(iter)\n",
    "\n",
    "a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    x = x - a * grad(x)\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x), ord=2)\n",
    "    iter += 1\n",
    "    error_list.append(error)\n",
    "    iter_list.append(iter)\n",
    "\n",
    "soln  # print the search trajectory\n",
    "print('The point in the plane is', 'x1 =', 1-2*soln[-1][0]-3*soln[-1][1], 'x2 =', soln[-1][0], 'x3 =', soln[-1][1])\n",
    "plt.plot(iter_list, error_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point in the plane is x1 = [-1.0714395] x2 = [-0.14244267] x3 = [0.78544161]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the Newton algorithm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def obj(x):\n",
    "    return 5 * x[0] ** 2 + 10 * x[1] ** 2 + 12 * x[0] * x[1] - 8 * x[0] - 14 * x[1] + 5\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([10 * x[0] + 12 * x[1] - 8, 20 * x[1] + 12 * x[0] - 14])\n",
    "\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = np.array([[0], [0]])  # initial guess\n",
    "iter = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[iter]  # start with the initial guess\n",
    "error = np.linalg.norm(grad(x), ord=2)\n",
    "\n",
    "error_list = list()\n",
    "iter_list = list()\n",
    "error_list.append(error)\n",
    "iter_list.append(iter)\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    phi = lambda a, x: obj(x) - a * 0.8 * np.matmul(grad(x).T, grad(x))  # define phi as a search criterion\n",
    "    while phi(a, x) < obj(x - a * grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5 * a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a * grad(x)\n",
    "    soln.append(x)\n",
    "    error = np.linalg.norm(grad(x), ord=2)\n",
    "    iter += 1\n",
    "    error_list.append(error)\n",
    "    iter_list.append(iter)\n",
    "\n",
    "soln  # print the search trajectory\n",
    "print('The point in the plane is', 'x1 =', 1-2*soln[-1][0]-3*soln[-1][1], 'x2 =', soln[-1][0], 'x3 =', soln[-1][1])\n",
    "plt.plot(iter_list, error_list)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "Assume $\\forall x_1, x_2 \\in \\mathcal{X}$, which has $x = \\lambda x_1 +(1-\\lambda)x_2$, $\\lambda \\in [0, 1]$. Let $h(x)=af(x)+bg(x)$ and we also know $f$ and $g$ is convex function. Then we have,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x)& =h(\\lambda x_1 +(1-\\lambda)x_2)\\\\ \n",
    "& \\leq a(\\lambda f(x_1) + (1 - \\lambda) f(x_2)) + b(\\lambda g(x_1) + (1 - \\lambda) g(x_2))\\\\\n",
    "& = \\lambda(af(x_1)+bg(x_1)) + (1-\\lambda)(af(x_2)+bg(x_2))\\\\ \n",
    "& = \\lambda h(x_1) + (1-\\lambda) h(x_2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Therefore, we can know that $h(x)=af(x)+bg(x)$ is the convex when $a>0$ and $b>0$.\n",
    "\n",
    "Let $h(x) = f(g(x))$, we know that $h(x)=f(g(x))$ is convex, so we can have $h^{\"}(x) \\geq 0$. We also know that $h^{\"}(x)=f^{\"}(g(x))g^{\"}(x)^2 + f^{'}(g(x))g^{\"}(x)$. From the condition, $f^{\"}\\geq 0$ and $g^{\"}\\geq 0$ because $f$ and $g$ is convex function. Hence, if we want $h^{\"}(x)=f^{\"}(g(x))g^{\"}(x)^2 + f^{'}(g(x))g^{\"}(x) \\geq 0$, it can find that $f^{'}(g(x)) \\geq 0$, which means that $f$ should be non-decreasing function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "We firstly prove that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ can derive that $f$ is convex function.\n",
    "Let $\\forall \\textbf{z} \\in \\mathcal{X}$, which has $\\textbf{z} = \\lambda \\textbf{x}_1 +(1-\\lambda)\\textbf{x}_0$, $\\lambda \\in [0, 1]$. Then we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\textbf{x}_1) \\geq f(\\textbf{z}) + g^T(\\textbf{z})(\\textbf{x}_1-\\textbf{z})\\\\\n",
    "f(\\textbf{x}_0) \\geq f(\\textbf{z}) + g^T(\\textbf{z})(\\textbf{x}_0-\\textbf{z})\n",
    "\\end{aligned}\n",
    "$$\n",
    "Consider $\\lambda f(\\textbf{x}_1) + (1-\\lambda)f(\\textbf{x}_0)$, it is easy to know that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lambda f(\\textbf{x}_1)+ (1-\\lambda)f(\\textbf{x}_0) & \\geq f(\\textbf{z}) + g^T(\\textbf{z})(\\lambda \\textbf{x}_1- \\lambda \\textbf{z} + (1-\\lambda)\\textbf{x}_0 - (1-\\lambda)\\textbf{z})\\\\\n",
    "& = f(\\textbf{z}) + g^T(\\textbf{z})(\\lambda \\textbf{x}_1 + (1-\\lambda)\\textbf{x}_0 - \\textbf{z})\\\\ \n",
    "& = f(\\textbf{z})\\\\\n",
    "& = f(\\lambda \\textbf{x}_1 +(1-\\lambda)\\textbf{x}_0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "It shows that $f(\\textbf{x})$ is the convex function\n",
    "\n",
    "Next, we start to prove that convex function $f(\\textbf{x})$ can derive $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$.\n",
    "\n",
    "From the definition of convex function, we can have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&f(\\lambda \\textbf{x}_1 + (1 - \\lambda)\\textbf{x}_0) \\leq \\lambda f(\\textbf{x}_1) + (1-\\lambda)f(\\textbf{x}_0), \\lambda \\in [0, 1]\\\\\n",
    "\\Rightarrow & f(\\lambda \\textbf{x}_1 + (1 - \\lambda)\\textbf{x}_0) - f(\\textbf{x}_0) \\leq \\lambda (f(\\textbf{x}_1) - f(\\textbf{x}_0))\\\\\n",
    "\\Rightarrow & \\frac{f(\\lambda \\textbf{x}_1 + (1 - \\lambda)\\textbf{x}_0) - f(\\textbf{x}_0)}{\\lambda(\\textbf{x}_1-\\textbf{x}_0)}(\\textbf{x}_1-\\textbf{x}_0) \\leq (f(\\textbf{x}_1) - f(\\textbf{x}_0))\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "take the limit on both sides of inequality\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Rightarrow & \\lim\\limits_{\\lambda\\rightarrow 0}\\frac{f(\\lambda \\textbf{x}_1 + (1 - \\lambda)\\textbf{x}_0) - f(\\textbf{x}_0)}{\\lambda(\\textbf{x}_1-\\textbf{x}_0)}(\\textbf{x}_1-\\textbf{x}_0) \\leq \\lim\\limits_{\\lambda\\rightarrow 0}(f(\\textbf{x}_1) - f(\\textbf{x}_0))\\\\\n",
    "\\Rightarrow & g_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)\\leq f(\\textbf{x}_1)-f(\\textbf{x}_0)\\\\\n",
    "\\Rightarrow & f(\\textbf{x}_1) \\geq f(\\textbf{x}_0) + \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0) \\Leftrightarrow$ $f(\\textbf{x})$ is convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.02,\n",
       " 0.039599999999999996,\n",
       " 0.058808,\n",
       " 0.07763184000000001,\n",
       " 0.0960792032,\n",
       " 0.114157619136,\n",
       " 0.13187446675328002,\n",
       " 0.14923697741821443,\n",
       " 0.16625223786985013,\n",
       " 0.18292719311245315,\n",
       " 0.19926864925020407,\n",
       " 0.2152832762652,\n",
       " 0.230977610739896,\n",
       " 0.2463580585250981,\n",
       " 0.2614308973545961,\n",
       " 0.2762022794075042,\n",
       " 0.2906782338193541,\n",
       " 0.304864669142967,\n",
       " 0.31876737576010766,\n",
       " 0.3323920282449055,\n",
       " 0.34574418768000736,\n",
       " 0.3588293039264072,\n",
       " 0.37165271784787907,\n",
       " 0.3842196634909215,\n",
       " 0.3965352702211031,\n",
       " 0.408604564816681,\n",
       " 0.4204324735203474,\n",
       " 0.43202382404994044,\n",
       " 0.44338334756894165,\n",
       " 0.4545156806175628,\n",
       " 0.46542536700521153,\n",
       " 0.4761168596651073,\n",
       " 0.4865945224718052,\n",
       " 0.4968626320223691,\n",
       " 0.5069253793819217,\n",
       " 0.5167868717942833,\n",
       " 0.5264511343583976,\n",
       " 0.5359221116712297,\n",
       " 0.5452036694378051,\n",
       " 0.554299596049049,\n",
       " 0.563213604128068,\n",
       " 0.5719493320455066,\n",
       " 0.5805103454045965,\n",
       " 0.5889001384965045,\n",
       " 0.5971221357265744,\n",
       " 0.6051796930120429,\n",
       " 0.6130760991518021,\n",
       " 0.620814577168766,\n",
       " 0.6283982856253907,\n",
       " 0.6358303199128829,\n",
       " 0.6431137135146252,\n",
       " 0.6502514392443327,\n",
       " 0.6572464104594461,\n",
       " 0.6641014822502572,\n",
       " 0.6708194526052521,\n",
       " 0.6774030635531471,\n",
       " 0.6838550022820841,\n",
       " 0.6901779022364424,\n",
       " 0.6963743441917135,\n",
       " 0.7024468573078793,\n",
       " 0.7083979201617217,\n",
       " 0.7142299617584872,\n",
       " 0.7199453625233174,\n",
       " 0.7255464552728511,\n",
       " 0.7310355261673941,\n",
       " 0.7364148156440462,\n",
       " 0.7416865193311653,\n",
       " 0.746852788944542,\n",
       " 0.7519157331656512,\n",
       " 0.7568774185023381,\n",
       " 0.7617398701322914,\n",
       " 0.7665050727296455,\n",
       " 0.7711749712750526,\n",
       " 0.7757514718495516,\n",
       " 0.7802364424125606,\n",
       " 0.7846317135643094,\n",
       " 0.7889390792930232,\n",
       " 0.7931602977071628,\n",
       " 0.7972970917530195,\n",
       " 0.8013511499179592,\n",
       " 0.8053241269196,\n",
       " 0.809217644381208,\n",
       " 0.8130332914935838,\n",
       " 0.8167726256637121,\n",
       " 0.8204371731504378,\n",
       " 0.8240284296874291,\n",
       " 0.8275478610936805,\n",
       " 0.8309969038718069,\n",
       " 0.8343769657943708,\n",
       " 0.8376894264784834,\n",
       " 0.8409356379489138,\n",
       " 0.8441169251899355,\n",
       " 0.8472345866861368,\n",
       " 0.8502898949524141,\n",
       " 0.8532840970533658,\n",
       " 0.8562184151122985,\n",
       " 0.8590940468100525,\n",
       " 0.8619121658738514,\n",
       " 0.8646739225563744,\n",
       " 0.867380444105247,\n",
       " 0.870032835223142,\n",
       " 0.8726321785186791,\n",
       " 0.8751795349483056,\n",
       " 0.8776759442493395,\n",
       " 0.8801224253643527,\n",
       " 0.8825199768570656,\n",
       " 0.8848695773199243,\n",
       " 0.8871721857735259,\n",
       " 0.8894287420580553,\n",
       " 0.8916401672168942,\n",
       " 0.8938073638725563,\n",
       " 0.8959312165951052,\n",
       " 0.8980125922632031,\n",
       " 0.900052340417939,\n",
       " 0.9020512936095803,\n",
       " 0.9040102677373887,\n",
       " 0.905930062382641,\n",
       " 0.9078114611349881,\n",
       " 0.9096552319122884,\n",
       " 0.9114621272740426,\n",
       " 0.9132328847285618,\n",
       " 0.9149682270339905,\n",
       " 0.9166688624933107,\n",
       " 0.9183354852434444,\n",
       " 0.9199687755385756,\n",
       " 0.9215694000278041,\n",
       " 0.923138012027248,\n",
       " 0.9246752517867031,\n",
       " 0.926181746750969,\n",
       " 0.9276581118159496,\n",
       " 0.9291049495796306,\n",
       " 0.9305228505880381,\n",
       " 0.9319123935762773,\n",
       " 0.9332741457047518,\n",
       " 0.9346086627906567,\n",
       " 0.9359164895348435,\n",
       " 0.9371981597441467,\n",
       " 0.9384541965492638,\n",
       " 0.9396851126182785,\n",
       " 0.940891410365913,\n",
       " 0.9420735821585947,\n",
       " 0.9432321105154228,\n",
       " 0.9443674683051143,\n",
       " 0.9454801189390121,\n",
       " 0.9465705165602318,\n",
       " 0.9476391062290273,\n",
       " 0.9486863241044468,\n",
       " 0.9497125976223578,\n",
       " 0.9507183456699106,\n",
       " 0.9517039787565125,\n",
       " 0.9526698991813822,\n",
       " 0.9536165011977545,\n",
       " 0.9545441711737994,\n",
       " 0.9554532877503235,\n",
       " 0.956344221995317,\n",
       " 0.9572173375554106,\n",
       " 0.9580729908043024,\n",
       " 0.9589115309882164,\n",
       " 0.9597333003684521,\n",
       " 0.960538634361083,\n",
       " 0.9613278616738613,\n",
       " 0.9621013044403841,\n",
       " 0.9628592783515765,\n",
       " 0.9636020927845449,\n",
       " 0.964330050928854,\n",
       " 0.9650434499102769,\n",
       " 0.9657425809120713,\n",
       " 0.9664277292938299,\n",
       " 0.9670991747079534,\n",
       " 0.9677571912137943,\n",
       " 0.9684020473895184,\n",
       " 0.969034006441728,\n",
       " 0.9696533263128935,\n",
       " 0.9702602597866355,\n",
       " 0.9708550545909028,\n",
       " 0.9714379534990848,\n",
       " 0.972009194429103,\n",
       " 0.972569010540521,\n",
       " 0.9731176303297107,\n",
       " 0.9736552777231164,\n",
       " 0.9741821721686541,\n",
       " 0.9746985287252811,\n",
       " 0.9752045581507754,\n",
       " 0.9757004669877599,\n",
       " 0.9761864576480047,\n",
       " 0.9766627284950447,\n",
       " 0.9771294739251437,\n",
       " 0.9775868844466409,\n",
       " 0.9780351467577081,\n",
       " 0.9784744438225539,\n",
       " 0.9789049549461029,\n",
       " 0.9793268558471808,\n",
       " 0.9797403187302373,\n",
       " 0.9801455123556325,\n",
       " 0.9805426021085198,\n",
       " 0.9809317500663495,\n",
       " 0.9813131150650225,\n",
       " 0.981686852763722,\n",
       " 0.9820531157084476,\n",
       " 0.9824120533942786,\n",
       " 0.9827638123263931,\n",
       " 0.9831085360798653,\n",
       " 0.9834463653582679,\n",
       " 0.9837774380511025,\n",
       " 0.9841018892900805,\n",
       " 0.9844198515042789,\n",
       " 0.9847314544741933,\n",
       " 0.9850368253847095,\n",
       " 0.9853360888770153,\n",
       " 0.985629367099475,\n",
       " 0.9859167797574855,\n",
       " 0.9861984441623358,\n",
       " 0.986474475279089,\n",
       " 0.9867449857735072,\n",
       " 0.9870100860580371,\n",
       " 0.9872698843368763,\n",
       " 0.9875244866501388,\n",
       " 0.9877739969171361,\n",
       " 0.9880185169787934,\n",
       " 0.9882581466392175,\n",
       " 0.9884929837064331,\n",
       " 0.9887231240323044,\n",
       " 0.9889486615516584,\n",
       " 0.9891696883206252,\n",
       " 0.9893862945542128,\n",
       " 0.9895985686631286,\n",
       " 0.989806597289866,\n",
       " 0.9900104653440687,\n",
       " 0.9902102560371873,\n",
       " 0.9904060509164435,\n",
       " 0.9905979298981147,\n",
       " 0.9907859713001523,\n",
       " 0.9909702518741493,\n",
       " 0.9911508468366663,\n",
       " 0.991327829899933,\n",
       " 0.9915012733019343,\n",
       " 0.9916712478358956,\n",
       " 0.9918378228791777,\n",
       " 0.9920010664215941,\n",
       " 0.9921610450931623,\n",
       " 0.992317824191299,\n",
       " 0.992471467707473,\n",
       " 0.9926220383533235,\n",
       " 0.992769597586257,\n",
       " 0.9929142056345319,\n",
       " 0.9930559215218413,\n",
       " 0.9931948030914045,\n",
       " 0.9933309070295764,\n",
       " 0.9934642888889849,\n",
       " 0.9935950031112052,\n",
       " 0.993723103048981,\n",
       " 0.9938486409880014,\n",
       " 0.9939716681682413,\n",
       " 0.9940922348048765,\n",
       " 0.994210390108779,\n",
       " 0.9943261823066034,\n",
       " 0.9944396586604713,\n",
       " 0.9945508654872619,\n",
       " 0.9946598481775166,\n",
       " 0.9947666512139663,\n",
       " 0.994871318189687,\n",
       " 0.9949738918258932,\n",
       " 0.9950744139893754,\n",
       " 0.9951729257095878,\n",
       " 0.9952694671953961,\n",
       " 0.9953640778514882,\n",
       " 0.9954567962944584,\n",
       " 0.9955476603685692,\n",
       " 0.9956367071611979,\n",
       " 0.9957239730179739,\n",
       " 0.9958094935576144,\n",
       " 0.9958933036864621,\n",
       " 0.9959754376127329,\n",
       " 0.9960559288604782,\n",
       " 0.9961348102832687,\n",
       " 0.9962121140776032,\n",
       " 0.9962878717960512,\n",
       " 0.9963621143601301,\n",
       " 0.9964348720729275,\n",
       " 0.996506174631469,\n",
       " 0.9965760511388396,\n",
       " 0.9966445301160628,\n",
       " 0.9967116395137415,\n",
       " 0.9967774067234667,\n",
       " 0.9968418585889973,\n",
       " 0.9969050214172174,\n",
       " 0.9969669209888731,\n",
       " 0.9970275825690956,\n",
       " 0.9970870309177137,\n",
       " 0.9971452902993594,\n",
       " 0.9972023844933722,\n",
       " 0.9972583368035047,\n",
       " 0.9973131700674346,\n",
       " 0.9973669066660859,\n",
       " 0.9974195685327641,\n",
       " 0.9974711771621089,\n",
       " 0.9975217536188667,\n",
       " 0.9975713185464894,\n",
       " 0.9976198921755596,\n",
       " 0.9976674943320484,\n",
       " 0.9977141444454074,\n",
       " 0.9977598615564992,\n",
       " 0.9978046643253692,\n",
       " 0.9978485710388618,\n",
       " 0.9978915996180846,\n",
       " 0.997933767625723,\n",
       " 0.9979750922732085,\n",
       " 0.9980155904277443,\n",
       " 0.9980552786191894,\n",
       " 0.9980941730468056,\n",
       " 0.9981322895858695,\n",
       " 0.9981696437941522,\n",
       " 0.9982062509182691,\n",
       " 0.9982421258999037,\n",
       " 0.9982772833819056,\n",
       " 0.9983117377142675,\n",
       " 0.9983455029599821,\n",
       " 0.9983785929007825,\n",
       " 0.9984110210427668,\n",
       " 0.9984428006219115,\n",
       " 0.9984739446094733,\n",
       " 0.9985044657172838,\n",
       " 0.9985343764029381,\n",
       " 0.9985636888748793,\n",
       " 0.9985924150973817,\n",
       " 0.9986205667954341,\n",
       " 0.9986481554595255,\n",
       " 0.998675192350335,\n",
       " 0.9987016885033283,\n",
       " 0.9987276547332617,\n",
       " 0.9987531016385965,\n",
       " 0.9987780396058246,\n",
       " 0.9988024788137081,\n",
       " 0.9988264292374339,\n",
       " 0.9988499006526852,\n",
       " 0.9988729026396315,\n",
       " 0.9988954445868389,\n",
       " 0.9989175356951021,\n",
       " 0.9989391849812,\n",
       " 0.998960401281576,\n",
       " 0.9989811932559445,\n",
       " 0.9990015693908256,\n",
       " 0.9990215380030091,\n",
       " 0.9990411072429489,\n",
       " 0.99906028509809,\n",
       " 0.9990790793961282,\n",
       " 0.9990974978082056,\n",
       " 0.9991155478520415,\n",
       " 0.9991332368950007,\n",
       " 0.9991505721571007,\n",
       " 0.9991675607139586,\n",
       " 0.9991842094996795,\n",
       " 0.9992005253096858,\n",
       " 0.9992165148034922,\n",
       " 0.9992321845074223,\n",
       " 0.9992475408172738,\n",
       " 0.9992625900009283,\n",
       " 0.9992773382009098,\n",
       " 0.9992917914368916,\n",
       " 0.9993059556081538,\n",
       " 0.9993198364959908,\n",
       " 0.9993334397660709,\n",
       " 0.9993467709707495,\n",
       " 0.9993598355513346,\n",
       " 0.9993726388403079,\n",
       " 0.9993851860635018,\n",
       " 0.9993974823422318,\n",
       " 0.9994095326953871,\n",
       " 0.9994213420414794,\n",
       " 0.9994329152006498,\n",
       " 0.9994442568966369,\n",
       " 0.9994553717587041,\n",
       " 0.99946626432353,\n",
       " 0.9994769390370595,\n",
       " 0.9994874002563183,\n",
       " 0.9994976522511919,\n",
       " 0.9995076992061681]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code for Problem 2\n",
    "\n",
    "obj = lambda x: (x - 1)**2  # note that this is 1D. In Prob 2 it should be 2D.\n",
    "grad = lambda x: 2*(x - 1)  # this is not the correct gradient!\n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = 0.  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = abs(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "#     a = line_search(x)\n",
    "    x = x - a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = abs(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
